---
title: "AS04_scraping_json"
author: "B06208001 龔泓愷"
date: "10/10/2019"
output:
  html_document:
    highlight: zenburn
    number_sections: no
    theme: cerulean
    toc: yes
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Loading packages
```{r library, message=FALSE, warning=FALSE}
library(tidyverse)
library(httr)
library(jsonlite)
library(lubridate)
options(stringsAsFactors = F)
```


# Scraping ubike data
- Here is the ubike json data link https://tcgbusfs.blob.core.windows.net/blobyoubike/YouBikeTP.json
- You are asked to scrape youbike data per 5 minutes, continuing at least 12 hours.
- Saving each ubike json per 5 minutes with file name containing timestamp, which means you should have at least 12*12 files

```{r hints}
url <- "https://tcgbusfs.blob.core.windows.net/blobyoubike/YouBikeTP.json"
GET(url) %>% content("text") %>% cat(file = "test.json")

# read_json("test.json") %>% class # Not work

readlines <- readLines("test.json", warn = FALSE)
parse <- fromJSON(readlines)

# Converting datetime to character
now() %>% format("%Y%m%d%H%M%S")

# Listing all files in a sub-folder
#list.files("data/", ".*\\.json") 

# If you pud your data in a sub-folder, you need to use full name to access them.
#list.files("data/", ".*\\.json", full.names = T) 
```

## Crawler
``` {r crawl, eval = FALSE}
for (i in 1:192) { # 以防萬一，抓 16 小時
  time1 <- now()
  Sys.sleep(280) # 20 秒緩衝時間
  
  while (TRUE) {
    time2 <- now()
    
    if (difftime(time2, time1, units='secs') >= 300) {
      break()
    }
  }
  message(i, "\t", now())
  now <- now() %>% format("%Y%m%d%H%M%S")
  GET(url) %>% content("text") %>% cat(file = str_c("data/", now, ".json"))
}
```

## **Q1.1 ANS** list files
- Using `list.files()` to list json files you scraped
- Using `length` to calculate `list.files()` result to see how many json files you have.

```{r}
json_file <- list.files("data/", ".*\\.json", full.names = T)
json_name <- list.files("data/", ".*\\.json")
length(json_file)
```

## Q1.2
- Reading JSON files one by one
- Converting them to data frame individually
- Defining one indicator: fullness = sbi/tot
- Calculating each site's fullness by time

### Thoughts
```{r thought, eval = FALSE}
readlines <- readLines(json_file[1], warn = FALSE)
parse <- fromJSON(readlines)[["retVal"]]
tmp <- lapply(parse, as.data.frame)

bike.tmp <- tmp[[1]] # 先引入首列為index

for (i in 1: length(tmp)) {
  bike.tmp <- rbind(bike.tmp, tmp[[i]])
}

bike.tmp <- bike.tmp[-1,] # 拿掉重複的首列
bike.tmp["json"] <- rep(json_name[1], 400) # 加入資料擷取時間
```

### Combine the thoughts
```{r tidy, eval=FALSE}
bike.df <- data.frame()

for (i in 1:length(json_file)) {
  # Read JSON file first
  readlines <- readLines(json_file[i], warn = FALSE)
  parse <- fromJSON(readlines)[["retVal"]]
  tmp <- lapply(parse, as.data.frame) # Convert to df
  
  # Generate DF for each json
  bike.tmp <- tmp[[1]] # 先引入首列為index
  for (j in 1: length(tmp)) {
    bike.tmp <- rbind(bike.tmp, tmp[[j]])
  }
  bike.tmp <- bike.tmp[-1,] # 拿掉重複的首列
  bike.tmp["json"] <- rep(json_name[i], 400) # 加入資料擷取時間
  
  # 整合每個 JSON 的 DF
  bike.df <- rbind(bike.df, bike.tmp)
}

write_rds(bike.df, "bike_df.rds")
```
```{r warning=FALSE, message=FALSE}
bike.df <- readRDS("bike_df.rds")

bike.site <- bike.df %>% group_by(json, sno) %>%
  summarise(sbi = sum(as.numeric(sbi)), 
            tot = sum(as.numeric(tot)),
            sna = sna, sarea = sarea) %>%
  mutate(fullness = sbi / tot,
         time = ymd_hms(str_sub(json, 1, 14)))

head(bike.site)
```

## **Q1.2 ANS**
- Using `geom_line` to display all site's fullness by time

```{r warning=FALSE, message=FALSE}
bike.plot <- bike.site %>%
  filter(sarea == "南港區") %>%
  filter(str_detect(sna, "捷運"))

ggplot(data = bike.plot, aes(x = time, y = fullness, col = sna)) +
  geom_line()
```

# Q2.Scraping dcard forum (No extra point if you have solved the Q1 succesfully)
- (If you cannot solve Q1 successfully, try to solve the Q2)
- Selecting one chatting forum and Scraping at least 3 pages by for-loop (You must find out the rule of url formatting, and use for-loop to scrap at least 3 pages back)
- e.g., https://www.dcard.tw/f/relationship
- One video tutorial has introduced how to find out dcard forum's JSON data page by page.
- Adding code chunks as you need below

## **Q2.1.ANS** Print out class and dimension of your data
```{r}
# your code here
```


## **Q2.2.ANS** Using bar chart to show the number of post trend by week.
```{r}
# your code here
```

# (No extra point) Discovering one more website generated by JSON
- Try to find another website or webpage which is generated by JSON.
- Website title: [Fill-in-here]
- Website url: [Fill-in-here]
- Selecting one chatting forum and Scraping at least 3 pages by for-loop (You must find out the rule of url formatting, and use for-loop to scrap at least 3 pages back)
- Adding code chunks as you need below

## **Q3.ANS** Print out glimpse(), class, and dimension of your data
```{r}
# YOUR CODE SHOULD BE HERE
```
